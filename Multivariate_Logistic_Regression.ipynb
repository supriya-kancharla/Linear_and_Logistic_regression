{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E3_A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqzqIsyQOwX4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMURZNS7QVcF"
      },
      "source": [
        "1. Read data and shuffle the rows in the raw data matrix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QWR6el3QsA7"
      },
      "source": [
        "2. Replace the responses 2 and 4 with 0 and 1 and divide the dataset into a training set and\n",
        "a test set. How many observations did you allocated for testing, and why this number?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWskBthDlAVc"
      },
      "source": [
        "data = pd.read_csv('/content/breast_cancer.csv', header=None)\n",
        "data1 = data.sample(frac=1).reset_index(drop=True)\n",
        "X = data1.iloc[:,0:9]\n",
        "y = data1.iloc[:,9]\n",
        "\n",
        "y = y.replace(2, 0)\n",
        "y = y.replace(4, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PPsraKTvaGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4a1711-8605-41ad-a9c8-3c59438c96d3"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "print(\"Number of observations for training data : \",len(X_train))\n",
        "print(\"Number of observations for test data : \",len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of observations for training data :  614\n",
            "Number of observations for test data :  69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tl1qEMfRBoD"
      },
      "source": [
        "3. Normalize the training data and train a linear logistic regression model using gradient\n",
        "descent. Print the hyperparameters \u000b and Niter and plot the cost function J(\f) as a\n",
        "function over iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVi_9rzVP6F4"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_normalized = pd.DataFrame(scaler.fit_transform(X_train))\n",
        "X_train_normalized_extended = pd.concat([pd.Series(1, index=X_train_normalized.index, name='00'), X_train_normalized], axis=1)\n",
        "\n",
        "X_test_normalized = pd.DataFrame(scaler.transform(X_test))\n",
        "X_test_normalized_extended = pd.concat([pd.Series(1, index=X_test_normalized.index, name='00'), X_test_normalized], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40VHitOC-kw1"
      },
      "source": [
        "def sigmoid(z):\n",
        "  \n",
        "    return 1/ (1 + np.exp(-z))\n",
        "\n",
        "def costFunction(theta, X, y):\n",
        "\n",
        "    m=len(y)    \n",
        "    predictions = sigmoid(np.dot(X,theta))\n",
        "    predictions = np.concatenate(predictions)\n",
        "    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))\n",
        "    cost = 1/m * sum(error)    \n",
        "    grad = 1/m * np.dot(X.transpose(),(predictions - y))\n",
        "    \n",
        "    return cost , grad\n",
        "\n",
        "def gradientDescent(X,y,theta,alpha,num_iters):\n",
        "\n",
        "    m=len(y)\n",
        "    J_history =[]\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "\n",
        "        cost, grad = costFunction(theta,X,y)\n",
        "        theta1 = np.concatenate(theta) - (alpha * grad).T\n",
        "        theta = np.array(theta1).reshape((n,1))\n",
        "        J_history.append(cost)\n",
        "    \n",
        "    return theta , J_history\n",
        "\n",
        "def classifierPredict(theta,X,y):\n",
        "    predictions = X.dot(theta)       \n",
        "    pred = predictions>0\n",
        "    pred = pred.astype(int)\n",
        "    count_ = sum(np.concatenate(np.array(pred)) == np.array(y))\n",
        "    accuracy = (count_ / len(X)) * 100\n",
        "    error_count = len(X) - count_\n",
        "    return accuracy, error_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "h1JUR-_R_AMw",
        "outputId": "6cad47ed-2456-4dff-ace9-37a35cfd8036"
      },
      "source": [
        "m , n = X_train_normalized_extended.shape[0], X_train_normalized_extended.shape[1]\n",
        "initial_theta = np.zeros((n,1))\n",
        "epochs = 10000\n",
        "lr = 0.001\n",
        "theta, cost_hist= gradientDescent(X_train_normalized_extended,y_train,initial_theta,lr,epochs)\n",
        "print(\"Hyperparameters :\\n-Alpha(learning rate) : {}\\n-Niter(number of iterations) : {}\".format(lr,epochs))\n",
        "print(\"Theta : \",np.concatenate(theta))\n",
        "plt.figure()\n",
        "plt.plot(range(0, len(cost_hist)), cost_hist,'b.')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters :\n",
            "-Alpha(learning rate) : 0.001\n",
            "-Niter(number of iterations) : 10000\n",
            "Theta :  [-0.45048171  0.56442365  0.56004388  0.58235531  0.50035033  0.46705731\n",
            "  0.68486017  0.55318671  0.50650671  0.30236871]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UgphgTxRRQU"
      },
      "source": [
        "4. What is the training error (number of non-correct classifications in the training data) and\n",
        "the training accuracy (percentage of correct classifications) for your model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkLoDyaqRfMC"
      },
      "source": [
        "5. What is the number of test error and the test accuracy for your model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YiqON7fFugr",
        "outputId": "4c47e4db-3150-4e37-ee7e-3c791384c23a"
      },
      "source": [
        "tr_acc, tr_err =classifierPredict(theta, X_train_normalized_extended, y_train)\n",
        "print(\"Train Accuracy : \", tr_acc)\n",
        "print(\"Train error : \", tr_err)\n",
        "\n",
        "print(\"-------------------\")\n",
        "\n",
        "te_acc, te_err =classifierPredict(theta, X_test_normalized_extended, y_test)\n",
        "print(\"Test Accuracy : \", te_acc)\n",
        "print(\"Test error : \", te_err)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy :  97.06840390879479\n",
            "Train error :  18\n",
            "-------------------\n",
            "Test Accuracy :  98.55072463768117\n",
            "Test error :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssX1cQ28Rr8C"
      },
      "source": [
        "6. Repeated runs will (due to the shuffling) give different results. Are they qualitatively\n",
        "the same? Do they depend on how many observations you put aside for testing? Is the\n",
        "difference between training and testing expected?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JHstp3HNCOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91e8f57-95e8-4a77-a871-9c160a4e191f"
      },
      "source": [
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "for k in range(10):\n",
        "    data1 = data.sample(frac=1).reset_index(drop=True)\n",
        "    X = data1.iloc[:,0:9]\n",
        "    y = data1.iloc[:,9]\n",
        "\n",
        "    y = y.replace(2, 0)\n",
        "    y = y.replace(4, 1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "    #print(\"Number of observations for training data : \",len(X_train))\n",
        "    #print(\"Number of observations for test data : \",len(X_test))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_normalized = pd.DataFrame(scaler.fit_transform(X_train))\n",
        "    X_train_normalized_extended = pd.concat([pd.Series(1, index=X_train_normalized.index, name='00'), X_train_normalized], axis=1)\n",
        "\n",
        "    X_test_normalized = pd.DataFrame(scaler.transform(X_test))\n",
        "    X_test_normalized_extended = pd.concat([pd.Series(1, index=X_test_normalized.index, name='00'), X_test_normalized], axis=1)\n",
        "\n",
        "    m , n = X_train_normalized_extended.shape[0], X_train_normalized_extended.shape[1]\n",
        "    initial_theta = np.zeros((n,1))\n",
        "    epochs = 10000\n",
        "    lr = 0.001\n",
        "    theta, cost_hist= gradientDescent(X_train_normalized_extended,y_train,initial_theta,lr,epochs)\n",
        "\n",
        "\n",
        "    tr_acc, tr_err =classifierPredict(theta, X_train_normalized_extended, y_train)\n",
        "    train_acc.append(tr_acc)\n",
        "    train_errors.append(tr_err)\n",
        "    # print(\"Train Accuracy : \", tr_acc)\n",
        "    # print(\"Train error : \", tr_err)\n",
        "\n",
        "    # print()\n",
        "\n",
        "    te_acc, te_err =classifierPredict(theta, X_test_normalized_extended, y_test)\n",
        "    # print(\"Test Accuracy : \", te_acc)\n",
        "    # print(\"Test error : \", te_err)\n",
        "    test_acc.append(te_acc)\n",
        "    test_errors.append(te_err)\n",
        "\n",
        "    print(\"*****************\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****************\n",
            "*****************\n",
            "*****************\n",
            "*****************\n",
            "*****************\n",
            "*****************\n",
            "*****************\n",
            "*****************\n",
            "*****************\n",
            "*****************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jMj_lGYLx5C",
        "outputId": "0daf0ea7-bc81-4498-f288-9d5676e41b59"
      },
      "source": [
        "print(\"Mean train accuracy : \", np.mean(train_acc))\n",
        "print(\"Mean test accuracy : \", np.mean(test_acc))\n",
        "print(\"Standard Deviation in train accuracy : \", np.std(train_acc))\n",
        "print(\"Standard Deviation in test accuracy : \", np.std(test_acc))\n",
        "\n",
        "print()\n",
        "print(\"Mean train errors : \", np.mean(train_errors))\n",
        "print(\"Mean test errors : \", np.mean(test_errors))\n",
        "print(\"Standard Deviation in train errors : \", np.std(train_errors))\n",
        "print(\"Standard Deviation in test errors : \", np.std(test_errors))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean train accuracy :  97.19869706840392\n",
            "Mean test accuracy :  97.10144927536233\n",
            "Standard Deviation in train accuracy :  0.17541253443434826\n",
            "Standard Deviation in test accuracy :  1.714805734231774\n",
            "\n",
            "Mean train errors :  17.2\n",
            "Mean test errors :  2.0\n",
            "Standard Deviation in train errors :  1.0770329614269007\n",
            "Standard Deviation in test errors :  1.1832159566199232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FS9MJPuOqc2"
      },
      "source": [
        "Due to shuffling, there might occurs different samples in train and test sets each time we run. Thus we have simulated the training and testing metrics for 10 iterations and the mean values are as reported above. The values are not exactly same for every iteration, but looking at the standard deviation, we could report a reasonable train and test accuracies for every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9WFIDEnOY0c",
        "outputId": "8593bace-0753-4d44-b3a7-c04c969330f5"
      },
      "source": [
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "for k in list([0.1,0.2,0.25,0.3,0.4,0.45,0.5,0.55,0.6,0.7]):\n",
        "    data1 = data.sample(frac=1).reset_index(drop=True)\n",
        "    X = data1.iloc[:,0:9]\n",
        "    y = data1.iloc[:,9]\n",
        "\n",
        "    y = y.replace(2, 0)\n",
        "    y = y.replace(4, 1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=k)\n",
        "    print(\"Number of observations for training data : \",len(X_train))\n",
        "    print(\"Number of observations for test data : \",len(X_test))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_normalized = pd.DataFrame(scaler.fit_transform(X_train))\n",
        "    X_train_normalized_extended = pd.concat([pd.Series(1, index=X_train_normalized.index, name='00'), X_train_normalized], axis=1)\n",
        "\n",
        "    X_test_normalized = pd.DataFrame(scaler.transform(X_test))\n",
        "    X_test_normalized_extended = pd.concat([pd.Series(1, index=X_test_normalized.index, name='00'), X_test_normalized], axis=1)\n",
        "\n",
        "    m , n = X_train_normalized_extended.shape[0], X_train_normalized_extended.shape[1]\n",
        "    initial_theta = np.zeros((n,1))\n",
        "    epochs = 10000\n",
        "    lr = 0.001\n",
        "    theta, cost_hist= gradientDescent(X_train_normalized_extended,y_train,initial_theta,lr,epochs)\n",
        "\n",
        "\n",
        "    tr_acc, tr_err =classifierPredict(theta, X_train_normalized_extended, y_train)\n",
        "    train_acc.append(tr_acc)\n",
        "    train_errors.append(tr_err)\n",
        "    print(\"Train Accuracy : \", tr_acc)\n",
        "    print(\"Train error : \", tr_err)\n",
        "\n",
        "    print()\n",
        "\n",
        "    te_acc, te_err =classifierPredict(theta, X_test_normalized_extended, y_test)\n",
        "    print(\"Test Accuracy : \", te_acc)\n",
        "    print(\"Test error : \", te_err)\n",
        "    test_acc.append(te_acc)\n",
        "    test_errors.append(te_err)\n",
        "\n",
        "    print(\"*****************\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of observations for training data :  614\n",
            "Number of observations for test data :  69\n",
            "Train Accuracy :  97.06840390879479\n",
            "Train error :  18\n",
            "\n",
            "Test Accuracy :  98.55072463768117\n",
            "Test error :  1\n",
            "*****************\n",
            "Number of observations for training data :  546\n",
            "Number of observations for test data :  137\n",
            "Train Accuracy :  96.88644688644689\n",
            "Train error :  17\n",
            "\n",
            "Test Accuracy :  98.54014598540147\n",
            "Test error :  2\n",
            "*****************\n",
            "Number of observations for training data :  512\n",
            "Number of observations for test data :  171\n",
            "Train Accuracy :  97.265625\n",
            "Train error :  14\n",
            "\n",
            "Test Accuracy :  97.07602339181285\n",
            "Test error :  5\n",
            "*****************\n",
            "Number of observations for training data :  478\n",
            "Number of observations for test data :  205\n",
            "Train Accuracy :  97.48953974895397\n",
            "Train error :  12\n",
            "\n",
            "Test Accuracy :  96.58536585365853\n",
            "Test error :  7\n",
            "*****************\n",
            "Number of observations for training data :  409\n",
            "Number of observations for test data :  274\n",
            "Train Accuracy :  97.79951100244499\n",
            "Train error :  9\n",
            "\n",
            "Test Accuracy :  96.35036496350365\n",
            "Test error :  10\n",
            "*****************\n",
            "Number of observations for training data :  375\n",
            "Number of observations for test data :  308\n",
            "Train Accuracy :  96.8\n",
            "Train error :  12\n",
            "\n",
            "Test Accuracy :  97.40259740259741\n",
            "Test error :  8\n",
            "*****************\n",
            "Number of observations for training data :  341\n",
            "Number of observations for test data :  342\n",
            "Train Accuracy :  97.0674486803519\n",
            "Train error :  10\n",
            "\n",
            "Test Accuracy :  97.36842105263158\n",
            "Test error :  9\n",
            "*****************\n",
            "Number of observations for training data :  307\n",
            "Number of observations for test data :  376\n",
            "Train Accuracy :  98.69706840390879\n",
            "Train error :  4\n",
            "\n",
            "Test Accuracy :  95.47872340425532\n",
            "Test error :  17\n",
            "*****************\n",
            "Number of observations for training data :  273\n",
            "Number of observations for test data :  410\n",
            "Train Accuracy :  97.06959706959707\n",
            "Train error :  8\n",
            "\n",
            "Test Accuracy :  97.3170731707317\n",
            "Test error :  11\n",
            "*****************\n",
            "Number of observations for training data :  204\n",
            "Number of observations for test data :  479\n",
            "Train Accuracy :  97.54901960784314\n",
            "Train error :  5\n",
            "\n",
            "Test Accuracy :  96.86847599164928\n",
            "Test error :  15\n",
            "*****************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gicr4stSRhEY",
        "outputId": "566489d9-1c4e-4fc5-d94d-2fc94473f84e"
      },
      "source": [
        "print(\"Mean train accuracy : \", np.mean(train_acc))\n",
        "print(\"Mean test accuracy : \", np.mean(test_acc))\n",
        "print(\"Standard Deviation in train accuracy : \", np.std(train_acc))\n",
        "print(\"Standard Deviation in test accuracy : \", np.std(test_acc))\n",
        "\n",
        "print()\n",
        "print(\"Mean train errors : \", np.mean(train_errors))\n",
        "print(\"Mean test errors : \", np.mean(test_errors))\n",
        "print(\"Standard Deviation in train errors : \", np.std(train_errors))\n",
        "print(\"Standard Deviation in test errors : \", np.std(test_errors))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean train accuracy :  97.41666742495886\n",
            "Mean test accuracy :  96.39460636519082\n",
            "Standard Deviation in train accuracy :  0.45069120217806946\n",
            "Standard Deviation in test accuracy :  1.027546579082154\n",
            "\n",
            "Mean train errors :  12.142857142857142\n",
            "Mean test errors :  7.428571428571429\n",
            "Standard Deviation in train errors :  3.0904725218262765\n",
            "Standard Deviation in test errors :  3.619674131234265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIvsME8OTfJd"
      },
      "source": [
        "No, the accuracy does not depend on how many observations you put aside for testing. This has been analysed by running the code for various values of the proportion for test set. \n",
        "\n",
        "The difference between training and testing results are not as expected. Usually we expect the test accuracy to be a bit lower than the training accuracy. But in case, they are very close and the reason for this might be the inbalanced dataset where, there are more samples with label 0 are greater than that of label 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3geODrAR7qS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}